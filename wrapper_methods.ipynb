{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Wrapper Methods\n",
    "Machine learning problems often involve datasets with many features. Some of those features might be very important for a specific machine learning model. Other features might be irrelevant. Given a feature set and a model, we would like to be able to distinguish between important and unimportant features (or even important combinations of features). Wrapper methods do exactly that.\n",
    "\n",
    "A wrapper method for feature selection is an algorithm that selects features by evaluating the performance of a machine learning model on different subsets of features. These algorithms add or remove features one at a time based on how useful those features are to the model.\n",
    "\n",
    "Wrapper methods have some advantages over filter methods. The main advantage is that wrapper methods evaluate features based on their performance with a specific model. Filter methods, on the other hand, can’t tell how important a feature is to a model.\n",
    "\n",
    "Another upside of wrapper methods is that they can take into account relationships between features. Sometimes certain features aren’t very useful on their own but instead perform well only when combined with other features. Since wrapper methods test subsets of features, they can account for those situations.\n",
    "\n",
    "This lesson will explain five different wrapper methods:\n",
    "\n",
    "Sequential forward selection\n",
    "Sequential backward selection\n",
    "Sequential forward floating selection\n",
    "Sequential backward floating selection\n",
    "Recursive feature elimination\n",
    "You’ll learn how to implement these algorithms in Python and evaluate the results.\n",
    "\n",
    "Before we get started, let’s take a look at a dataset that you’ll use throughout this lesson.\n",
    "\n",
    "Instructions\n",
    "\n",
    "The data in the workspace was taken from the UCI Machine Learning Repository. The outcome variable, Classification, is a 1 if a patient has breast cancer and a 0 if not. The features are Age, BMI (body mass index), Glucose, Insulin, HOMA (homeostatic model assessment, a measure of insulin resistance), as well as four different protein levels: Leptin, Adiponectin, Resistin, and MCP.1.\n",
    "\n",
    "Uncomment print(health.head()) and take a look at the data.\n",
    "\n",
    "What are some pros and cons of having this many features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>HOMA</th>\n",
       "      <th>Leptin</th>\n",
       "      <th>Adiponectin</th>\n",
       "      <th>Resistin</th>\n",
       "      <th>MCP.1</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>70</td>\n",
       "      <td>2.707</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>8.8071</td>\n",
       "      <td>9.702400</td>\n",
       "      <td>7.99585</td>\n",
       "      <td>417.114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>20.690495</td>\n",
       "      <td>92</td>\n",
       "      <td>3.115</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>5.429285</td>\n",
       "      <td>4.06405</td>\n",
       "      <td>468.786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>23.124670</td>\n",
       "      <td>91</td>\n",
       "      <td>4.498</td>\n",
       "      <td>1.009651</td>\n",
       "      <td>17.9393</td>\n",
       "      <td>22.432040</td>\n",
       "      <td>9.27715</td>\n",
       "      <td>554.697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>21.367521</td>\n",
       "      <td>77</td>\n",
       "      <td>3.226</td>\n",
       "      <td>0.612725</td>\n",
       "      <td>9.8827</td>\n",
       "      <td>7.169560</td>\n",
       "      <td>12.76600</td>\n",
       "      <td>928.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>92</td>\n",
       "      <td>3.549</td>\n",
       "      <td>0.805386</td>\n",
       "      <td>6.6994</td>\n",
       "      <td>4.819240</td>\n",
       "      <td>10.57635</td>\n",
       "      <td>773.920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age        BMI  Glucose  Insulin      HOMA   Leptin  Adiponectin  Resistin  \\\n",
       "0   48  23.500000       70    2.707  0.467409   8.8071     9.702400   7.99585   \n",
       "1   83  20.690495       92    3.115  0.706897   8.8438     5.429285   4.06405   \n",
       "2   82  23.124670       91    4.498  1.009651  17.9393    22.432040   9.27715   \n",
       "3   68  21.367521       77    3.226  0.612725   9.8827     7.169560  12.76600   \n",
       "4   86  21.111111       92    3.549  0.805386   6.6994     4.819240  10.57635   \n",
       "\n",
       "     MCP.1  Classification  \n",
       "0  417.114               1  \n",
       "1  468.786               1  \n",
       "2  554.697               1  \n",
       "3  928.220               1  \n",
       "4  773.920               1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "health = pd.read_csv(\"data/dataR2.csv\")\n",
    "\n",
    "health.head()\n",
    "#print(health.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up a Logistic Regression Model\n",
    "Before we can use a wrapper method, we need to specify a machine learning model. We’ll train a logistic regression model on the health data and see how well it performs.\n",
    "\n",
    "We’ll prepare the data by splitting it into a pandas DataFrame X and a pandas Series y. X will contain the observations of the independent variables, and y will contain the observations of the dependent variable.\n",
    "\n",
    "Here’s an example of how to do this. The fire dataset below was taken from the UCI Machine Learning Repository and cleaned for our analysis. Its features are Temperature, RH (relative humidity), Ws (wind speed), Rain, DMC (Duff Moisture Code), and FWI (Fire Weather Index). The final column, Classes, contains a 1 if there is a forest fire at a specific location on a given day and 0 if not.\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "# Load the data\n",
    "fire = pd.read_csv(\"fire.csv\")\n",
    "# Split independent and dependent variables\n",
    "X = fire.iloc[:,:-1]\n",
    "y = fire.iloc[:,-1]\n",
    "We can create a logistic regression model and fit it to X and y with scikit-learn using the following code.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " \n",
    "# Create and fit the logistic regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "Logistic regression models give a probability that an observation belongs to a category. In the fire dataset, probabilities greater than 0.5 are considered predictions that there is a fire, and probabilities less than 0.5 are considered predictions that there is no fire. In the health dataset, probabilities greater than 0.5 are considered predictions that a patient has breast cancer.\n",
    "\n",
    "The accuracy of a logistic regression model is the percentage of correct predictions that it makes on a testing set. In scikit-learn, you can check the accuracy of a model with the .score() method.\n",
    "\n",
    "print(lr.score(X,y))\n",
    "This outputs:\n",
    "\n",
    "0.9836065573770492\n",
    "For our testing set, our logistic regression model correctly predicts whether a fire occurred 98.4% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8017241379310345\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the data\n",
    "health = pd.read_csv(\"data/dataR2.csv\")\n",
    "# Split independent and dependent variables\n",
    "X = health.iloc[:,:-1]\n",
    "y = health.iloc[:,-1]\n",
    "\"\"\"The dataset health has been loaded and split into features X and outcome y for you in script.py. Use the .fit() method to fit lr to X and y.\n",
    "\n",
    "(Side note: scikit-learn uses an algorithm that goes through many iterations to find optimal logistic regression coefficients. For this particular dataset, the algorithm doesn’t converge after the default maximum number of iterations, so we’ve included max_iter=1000 as a parameter in the LogisticRegression object just to make sure the algorithm converges.)\"\"\"\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model\n",
    "\"\"\"Use the .score() method to print the accuracy of the model and print it. How often does this model correctly predict whether or not a patient has breast cancer?\"\"\"\n",
    "# Print the accuracy of the model\n",
    "lr.fit(X,y)\n",
    "print(lr.score(X,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Forward Selection\n",
    "Now that we have a specific machine learning model, we can use a wrapper method to choose a smaller feature subset.\n",
    "\n",
    "Sequential forward selection is a wrapper method that builds a feature set by starting with no features and then adding one feature at a time until a desired number of features is reached. In the first step, the algorithm will train and test a model using only one feature at a time. The algorithm keeps the feature that performs best.\n",
    "\n",
    "In each subsequent step, the algorithm will test the model on each possible new feature addition. Whichever feature improves model performance the most is then added to the feature subset. This process stops once we have the desired number of features.\n",
    "\n",
    "Let’s say we want to use three features, and we have five features to choose from: age, height, weight, blood_pressure, and resting_heart_rate. Sequential forward selection will train your machine learning model on five different feature subsets: one for each feature.\n",
    "\n",
    "If the model performs best on the subset {age}, the algorithm will then train and test the model on the following four subsets:\n",
    "\n",
    "{age, height}\n",
    "{age, weight}\n",
    "{age, blood_pressure}\n",
    "{age, resting_heart_rate}\n",
    "If the model performs best on {age, resting_heart_rate}, the algorithm will test the model on the following three subsets:\n",
    "\n",
    "{age, height, resting_heart_rate}\n",
    "{age, weight, resting_heart_rate}\n",
    "{age, blood_pressure, resting_heart_rate}\n",
    "If the model performs best on {age, weight, resting_heart_rate}, it will stop the algorithm and use that feature set.\n",
    "\n",
    "Sequential forward selection is a greedy algorithm: instead of checking every possible feature set by brute force, it adds whichever feature gives the best immediate performance gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used sequential forward selection to add a fourth feature to the feature set {age, weight, resting_heart_rate} that you saw in the preceding example, what two feature sets would the algorithm test in the next step? Create sets called set1 and set2 that contain the appropriate features.\n",
    "\n",
    "For example, if the answer was {height, resting_heart_rate} and {weight, resting_heart_rate}, you would type:\n",
    "\n",
    "set1 = {\"height\", \"resting_heart_rate\"}\n",
    "set2 = {\"weight\", \"resting_heart_rate\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = {\"age\", \"height\", \"weight\", \"resting_heart_rate\"}\n",
    "set2 = {\"age\", \"weight\", \"blood_pressure\", \"resting_heart_rate\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Forward Selection with mlxtend\n",
    "Recall from a previous exercise that the logistic regression model was about 80.2% accurate at predicting if a patient had breast cancer. But there were NINE different features. Are all of those features necessary, or is it possible that the model could make accurate predictions with fewer features? That would make the model easier to understand, and it could simplify diagnosis.\n",
    "\n",
    "We will use the SFS class from Python’s mlxtend library to implement sequential forward selection and choose a subset of just THREE features for the logistic regression model that we used earlier.\n",
    "\n",
    "# Set up SFS parameters\n",
    "sfs = SFS(lr,\n",
    "           k_features=3, # number of features to select\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           scoring='accuracy',\n",
    "           cv=0)\n",
    "# Fit SFS to our features X and outcome y   \n",
    "sfs.fit(X, y)\n",
    "The first parameter is the name of the model that you’re using. In the previous exercise, we called the logistic regression model lr.\n",
    "The parameter k_features determines how many features the algorithm will select.\n",
    "forward=True and floating=False ensure that we are using sequential forward selection.\n",
    "scoring determines how the algorithm will evaluate each feature subset. It’s often okay to use the default value None because mlxtend will automatically use a metric that is suitable for whatever scikit-learn model you are using. For this lesson, we’ll set it to 'accuracy'.\n",
    "cv allows you to do k-fold cross-validation. We’ll leave it at 0 for this lesson and only evaluate performance on the training set.\n",
    "We’ll see which features were selected in the next exercise. For now, we just want to fit the SFS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use mlxtend to create an SFS object called sfs. Set it to select three features, and make sure that you set forward and floating arguments to ensure that you are using sequential forward selection. Also set scoring='accuracy' and cv=0.\n",
    "\n",
    "Checkpoint 2 Passed\n",
    "2. Use the .fit() method to fit sfs to X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SequentialFeatureSelector(cv=0, estimator=LogisticRegression(max_iter=1000),\n",
       "                          k_features=3, scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SequentialFeatureSelector</label><div class=\"sk-toggleable__content\"><pre>SequentialFeatureSelector(cv=0, estimator=LogisticRegression(max_iter=1000),\n",
       "                          k_features=3, scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "SequentialFeatureSelector(cv=0, estimator=LogisticRegression(max_iter=1000),\n",
       "                          k_features=3, scoring='accuracy')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Load the data\n",
    "health = pd.read_csv(\"data/dataR2.csv\")\n",
    "X = health.iloc[:,:-1]\n",
    "y = health.iloc[:,-1]\n",
    "\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Sequential forward selection\n",
    "sfs = SFS(lr,\n",
    "k_features=3, # number of features to select\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           scoring='accuracy',\n",
    "           cv=0\n",
    "\n",
    ")\n",
    "# Fit the equential forward selection model\n",
    "# Fit SFS to our features X and outcome y   \n",
    "sfs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Result of Sequential Forward Selection\n",
    "The sfs object that you fit in the previous exercise contains information about the sequential forward selection that was applied to your feature set. The .subsets_ attribute allows you to see much of that information, including which feature was chosen at each step and the model’s accuracy after each feature addition.\n",
    "\n",
    "sfs.subsets_ is a dictionary that looks something like this:\n",
    "\n",
    "{1: {'feature_idx': (7,),\n",
    "  'cv_scores': array([0.93852459]),\n",
    "  'avg_score': 0.9385245901639344,\n",
    "  'feature_names': ('FWI',)},\n",
    " 2: {'feature_idx': (4, 7),\n",
    "  'cv_scores': array([0.97540984]),\n",
    "  'avg_score': 0.9754098360655737,\n",
    "  'feature_names': ('DMC', 'FWI')},\n",
    " 3: {'feature_idx': (1, 4, 7),\n",
    "  'cv_scores': array([0.9795082]),\n",
    "  'avg_score': 0.9795081967213115,\n",
    "  'feature_names': (' RH', 'DMC', 'FWI')}}\n",
    "The keys in this dictionary are the numbers of features at each step in the sequential forward selection algorithm. The values in the dictionary are dictionaries with information about the feature set at each step. 'avg_score' is the accuracy of the model with the specified number of features.\n",
    "\n",
    "In this particular example, the model had an accuracy of about 93.9% after the feature FWI was added. The accuracy improved to about 97.5% after a second feature, DMC, was added. Once three features were added the accuracy improved to about 98.0%.\n",
    "\n",
    "You can use this dictionary to easily get a tuple of chosen features or the accuracy of the model after any step.\n",
    "\n",
    "# Print a tuple of feature names after 5 features are added\n",
    "print(sfs.subsets_[5]['feature_names'])\n",
    "This outputs:\n",
    "\n",
    "(' RH', ' Ws', 'Rain ', 'DMC', 'FWI')\n",
    "# Print the accuracy of the model after 5 features are added\n",
    "print(sfs.subsets_[5]['avg_score'])\n",
    "This outputs:\n",
    "\n",
    "0.9836065573770492\n",
    "The mlxtend library also makes it easy to visualize how the accuracy of a model changes as sequential forward selection adds features. You can use the code plot_sfs(sfs.get_metric_dict()) to create a matplotlib figure that plots the model’s performance as a function of the number of features used.\n",
    "\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Plot the accuracy of the model as a function of the number of features\n",
    "plot_sfs(sfs.get_metric_dict())\n",
    "plt.show()\n",
    "Model accuracy\n",
    "\n",
    "This plot shows you some of the same information that was in sfs.subsets_. The accuracy after one feature was added is about 93.9%, then 97.5% after a second feature is added, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Age', 'Glucose', 'Insulin')\n",
      "0.7672413793103449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\momad\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\momad\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "health = pd.read_csv(\"data/dataR2.csv\")\n",
    "X = health.iloc[:,:-1]\n",
    "y = health.iloc[:,-1]\n",
    "\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Sequential forward selection\n",
    "sfs = SFS(lr,\n",
    "          k_features=3,\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring='accuracy',\n",
    "          cv=0)\n",
    "sfs.fit(X, y)\n",
    "\n",
    "\"\"\"Use the .subsets_ attribute of sfs to print a tuple that contains the names of the features chosen in the previous exercise (remember that we chose three features).\"\"\"\n",
    "# Print the chosen feature names\n",
    "print(sfs.subsets_[3]['feature_names'])\n",
    "# Print the accuracy of the model after sequential forward selection\n",
    "\"\"\"Use the .subsets_ attribute of sfs to print the accuracy of the model with three features after doing sequential forward selection.\n",
    "\n",
    "Recall that the original model was about 80% accurate when it used all nine features. How does the accuracy of the model with only three features compare?\"\"\"\n",
    "print(sfs.subsets_[3]['avg_score'])\n",
    "# Plot the model accuracy\n",
    "\"\"\"Use plot_sfs(sfs.get_metric_dict()) to plot the accuracy as a function of the number of features. Remember to show your plot.\"\"\"\n",
    "plot_sfs(sfs.get_metric_dict())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Backward Selection with mlxtend\n",
    "Sequential backward selection is another wrapper method for feature selection. It is very similar to sequential forward selection, but there is one key difference. Instead of starting with no features and adding one feature at a time, sequential backward selection starts with all of the available features and removes one feature at a time.\n",
    "\n",
    "Let’s again say we want to use three of the following five features: age, height, weight, blood_pressure, and resting_heart_rate. Sequential backward selection will start by training whatever machine learning model you are using on five different feature subsets, one for each possible feature removal:\n",
    "\n",
    "{height, weight, blood_pressure, resting_heart_rate}\n",
    "{age, weight, blood_pressure, resting_heart_rate}\n",
    "{age, height, blood_pressure, resting_heart_rate}\n",
    "{age, height, weight, resting_heart_rate}\n",
    "{age, height, weight, blood_pressure}\n",
    "Let’s say that out of the five subsets, the model performed best on the subset without blood_pressure. Then the algorithm will proceed with the feature set {age, height, weight, resting_heart_rate}. It then tries removing each of age, height, weight, and resting_heart_rate.\n",
    "\n",
    "Let’s say that of those four subsets, the model performed best without weight. Then it will arrive at the subset {age, height, resting_heart_rate}. The algorithm will stop there since it arrived at the desired number of features.\n",
    "\n",
    "To implement sequential backward selection in mlxtend you can use the same SFS class you used for sequential forward selection. The only difference is that you have to set the parameter forward to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SequentialFeatureSelector(cv=0, estimator=LogisticRegression(max_iter=1000),\n",
       "                          forward=False, k_features=3, scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SequentialFeatureSelector</label><div class=\"sk-toggleable__content\"><pre>SequentialFeatureSelector(cv=0, estimator=LogisticRegression(max_iter=1000),\n",
       "                          forward=False, k_features=3, scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "SequentialFeatureSelector(cv=0, estimator=LogisticRegression(max_iter=1000),\n",
       "                          forward=False, k_features=3, scoring='accuracy')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Load the data\n",
    "health = pd.read_csv(\"data/dataR2.csv\")\n",
    "X = health.iloc[:,:-1]\n",
    "y = health.iloc[:,-1]\n",
    "\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\"\"\"The object sbs is currently set to use sequential forward selection. Change the forward parameter so that it uses sequential backward selection instead. You can leave all the other parameters the same.\"\"\"\n",
    "# Sequential backward selection\n",
    "sbs = SFS(lr,\n",
    "          k_features=3,\n",
    "          forward=False,\n",
    "          floating=False,\n",
    "          scoring='accuracy',\n",
    "          cv=0)\n",
    "\n",
    "# Fit the sequential backward selection model\n",
    "sbs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Result of Sequential Backward Selection\n",
    "As you learned in a previous exercise, model.subsets_ is a dictionary that contains information about feature subsets from each step of an SFS selection model. This works with sequential backward selection just like it did with sequential forward selection.\n",
    "\n",
    "print(sbs.subsets_)\n",
    "{6: {'feature_idx': (0, 1, 2, 3, 4, 5),\n",
    "  'cv_scores': array([0.98360656]),\n",
    "  'avg_score': 0.9836065573770492,\n",
    "  'feature_names': ('Temperature', ' RH', ' Ws', 'Rain ', 'DMC', 'FWI')},\n",
    " 5: {'feature_idx': (1, 2, 3, 4, 5),\n",
    "  'cv_scores': array([0.98360656]),\n",
    "  'avg_score': 0.9836065573770492,\n",
    "  'feature_names': (' RH', ' Ws', 'Rain ', 'DMC', 'FWI')},\n",
    " 4: {'feature_idx': (2, 3, 4, 5),\n",
    "  'cv_scores': array([0.98360656]),\n",
    "  'avg_score': 0.9836065573770492,\n",
    "  'feature_names': (' Ws', 'Rain ', 'DMC', 'FWI')},\n",
    " 3: {'feature_idx': (2, 4, 5),\n",
    "  'cv_scores': array([0.9795082]),\n",
    "  'avg_score': 0.9795081967213115,\n",
    "  'feature_names': (' Ws', 'DMC', 'FWI')}}\n",
    "You can also use plot_sfs(sfs.get_metric_dict()) to visualize the results of sequential backward selection.\n",
    "\n",
    "# Plot the accuracy of the model as a function of the number of features\n",
    "plot_sfs(sbs.get_metric_dict())\n",
    "plt.show()\n",
    "Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Age', 'Glucose', 'Resistin')\n",
      "0.7413793103448276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\momad\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\momad\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\_methods.py:256: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "health = pd.read_csv(\"data/dataR2.csv\")\n",
    "X = health.iloc[:,:-1]\n",
    "y = health.iloc[:,-1]\n",
    "\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Sequential backward selection\n",
    "sbs = SFS(lr,\n",
    "          k_features=3,\n",
    "          forward=False,\n",
    "          floating=False,\n",
    "          scoring='accuracy',\n",
    "          cv=0)\n",
    "sbs.fit(X, y)\n",
    "\n",
    "\"\"\"Use the .subsets_ attribute of sbs to print a tuple that contains the names of the features chosen in the previous exercise (remember that we chose three features). Did you get the same feature set as you did with sequential forward selection?\"\"\"\n",
    "# Print the chosen feature names\n",
    "print(sbs.subsets_[3]['feature_names'])\n",
    "\n",
    "\"\"\"Use the .subsets_ attribute of sbs to print the accuracy of the model after doing sequential backward selection.\n",
    "\n",
    "Recall that the original model was about 80% accurate when it used all nine features. How does the accuracy of the model with only three features compare? How does the accuracy of sequential backward selection compare to sequential forward selection?\"\"\"\n",
    "# Print the accuracy of the model after sequential backward selection\n",
    "print(sbs.subsets_[3]['avg_score'])\n",
    "\n",
    "\"\"\"Use plot_sfs(sbs.get_metric_dict()) to plot the accuracy as a function of the number of features. Notice that the accuracy sometimes decreases when a feature is removed. Do you think that too much accuracy was lost, or is that a reasonable trade-off to have a simpler model?\"\"\"\n",
    "# Plot the model accuracy\n",
    "plot_sfs(sbs.get_metric_dict())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Forward and Backward Floating Selection\n",
    "Sequential forward floating selection is a variation of sequential forward selection. It starts with zero features and adds one feature at a time, just like sequential forward selection, but after each addition, it checks to see if we can improve performance by removing a feature.\n",
    "\n",
    "If performance can’t be improved, the floating algorithm will continue to the next step and add another feature.\n",
    "If performance can be improved, the algorithm will make the removal that improves performance the most (unless removal of that feature would lead to an infinite loop of adding and removing the same feature over and over again).\n",
    "For example, let’s say that the algorithm has just added weight to the feature set {age, resting_heart_rate}, resulting in the set {age, weight, resting_heart_rate}. The floating algorithm will test whether it can improve performance by removing age or resting_heart_rate. If the removal of age improves performance, then the algorithm will proceed with the set {weight, resting_heart_rate}.\n",
    "\n",
    "Sequential backward floating selection works similarly. Starting with all available features, it removes one feature at a time. After each feature removal, it will check to see if any feature additions will improve performance (but it won’t add any features that would result in an infinite loop).\n",
    "\n",
    "Floating selection algorithms are sometimes preferable to their non-floating counterparts because they test the model on more possible feature subsets. They can detect useful relationships between variables that plain sequential forward and backward selection might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say that sequential backward floating selection is being done on the set {age, height, weight, blood_pressure, resting_heart_rate}. The algorithm removes weight, blood_pressure, and resting_heart_rate in that order. After removing resting_heart_rate, the model had an accuracy of 86%. Now it will try some feature additions. The possible feature additions give the following accuracies:\n",
    "\n",
    "Feature subset\tAccuracy\n",
    "{age, height, weight}\t0.95\n",
    "{age, height, blood_pressure}\t0.92\n",
    "\n",
    "Will it add a feature?\n",
    "\n",
    "If yes, create a variable called added_feature, and set it equal to the name of the feature as a string.\n",
    "If no, create a variable called added_feature, and set it equal to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_feature = 'weight'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Forward and Backward Floating Selection with mlxtend\n",
    "We can implement sequential forward or backward floating selection in mlxtend by setting the parameter floating to True. The parameter forward determines whether mlxtend will use sequential forward floating selection or sequential backward floating selection. As usual, the dictionary model.subsets_ will contain useful information about the chosen features.\n",
    "\n",
    "Here’s an implementation of sequential backward floating selection.\n",
    "\n",
    "# Sequential backward floating selection\n",
    "sbfs = SFS(lr,\n",
    "          k_features=5,\n",
    "          forward=False,\n",
    "          floating=True,\n",
    "          scoring='accuracy',\n",
    "          cv=0)\n",
    "sbfs.fit(X, y)\n",
    "We can use the .subsets_ attribute to look at feature names, just like we did with the non-floating sequential selection algorithms.\n",
    "\n",
    "print(sbfs.subsets_[5]['feature_names'])\n",
    "This outputs:\n",
    "\n",
    "(' RH', ' Ws', 'Rain ', 'DMC', 'FWI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Age', 'Glucose', 'Insulin')\n",
      "('Age', 'Glucose', 'Resistin')\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Load the data\n",
    "health = pd.read_csv(\"data/dataR2.csv\")\n",
    "X = health.iloc[:,:-1]\n",
    "y = health.iloc[:,-1]\n",
    "\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Sequential forward floating selection\n",
    "sffs = SFS(lr,\n",
    "          k_features=3,\n",
    "          forward=True,\n",
    "          floating=True,\n",
    "          scoring='accuracy',\n",
    "          cv=0)\n",
    "sffs.fit(X, y)\n",
    "\n",
    "# Print a tuple with the names of the features chosen by sequential forward floating selection.\n",
    "print(sffs.subsets_[3]['feature_names'])\n",
    "# Sequential backward floating selection\n",
    "sbfs = SFS(lr,\n",
    "          k_features=3,\n",
    "          forward=False,\n",
    "          floating=True,\n",
    "          scoring='accuracy',\n",
    "          cv=0)\n",
    "sbfs.fit(X, y)\n",
    "\n",
    "# Print a tuple with the names of the features chosen by sequential backward floating selection.\n",
    "print(print(sbfs.subsets_[3]['feature_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "Recursive feature elimination is another wrapper method for feature selection. It starts by training a model with all available features. It then ranks each feature according to an importance metric and removes the least important feature. The algorithm then trains the model on the smaller feature set, ranks those features, and removes the least important one. The process stops when the desired number of features is reached.\n",
    "\n",
    "In regression problems, features are ranked by the size of the absolute value of their coefficients. For example, let’s say that we trained a regression model with five features and got the following regression coefficients.\n",
    "\n",
    "Feature\tRegression coefficient\n",
    "age\t2.5\n",
    "height\t7.0\n",
    "weight\t-4.3\n",
    "blood_pressure\t-5.7\n",
    "resting_heart_rate\t-4.6\n",
    "\n",
    "The regression coefficient for age has the smallest absolute value, so it is ranked least important by recursive feature elimination. It will be removed, and the remaining four features will be re-ranked after the model is trained without age.\n",
    "\n",
    "It’s important to note that you might need to standardize data before doing recursive feature elimination. In regression problems in particular, it’s necessary to standardize data so that the scale of features doesn’t affect the size of the coefficients.\n",
    "\n",
    "Note that recursive feature elimination is different from sequential backward selection. Sequential backward selection removes features by training a model on a collection of subsets (one for each possible feature removal) and greedily proceeding with whatever subset performs best. Recursive feature elimination, on the other hand, only trains a model on one feature subset before deciding which feature to remove next.\n",
    "\n",
    "This is one advantage of recursive feature elimination. Since it only needs to train and test a model on one feature subset per feature removal, it can be much faster than the sequential selection methods that we’ve covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Say you want to choose four out of six possible features for a model. If you use recursive feature elimination, how many different feature subsets will you test a model on? Set this number to a variable called rfe_test_count.\n",
    "\n",
    "Checkpoint 2 Passed\n",
    "\n",
    "2. If you use sequential backward selection to choose four out of six features, how many different feature subsets will you test a model on? Set this number to a variable called sbs_test_count. Which method uses fewer tests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_test_count = 2\n",
    "sbs_test_count = 6 + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination with scikit-learn\n",
    "We can use scikit-learn to implement recursive feature elimination. Since we’re using a logistic regression model, it’s important to standardize data before we proceed.\n",
    "\n",
    "We can standardize features using scikit-learn’s StandardScaler().\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "X = StandardScaler().fit_transform(X)\n",
    "Once the data is standardized, you can train the model and do recursive feature elimination using RFE() from scikit-learn. As before with the sequential feature selection methods, you have to specify a scikit-learn model for the estimator parameter (in this case, lr for our logistic regression model). n_features_to_select is self-explanatory: set it to the number of features you want to select.\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    " \n",
    "# Recursive feature elimination\n",
    "rfe = RFE(lr, n_features_to_select=2)\n",
    "rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RFE(estimator=LogisticRegression(max_iter=1000), n_features_to_select=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RFE</label><div class=\"sk-toggleable__content\"><pre>RFE(estimator=LogisticRegression(max_iter=1000), n_features_to_select=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RFE(estimator=LogisticRegression(max_iter=1000), n_features_to_select=3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "health = pd.read_csv(\"data/dataR2.csv\")\n",
    "X = np.array(health.iloc[:,:-1])\n",
    "y = np.array(health.iloc[:,-1])\n",
    "\n",
    "# Standardize the data\n",
    "\"\"\"Standardize the data by uncommenting the line X = StandardScaler().fit_transform(X).\"\"\"\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\"\"\"Use sklearn to create an RFE object called rfe that chooses 3 features.\"\"\"\n",
    "# Recursive feature elimination\n",
    "\"\"\"Use the .fit() method on rfe to fit the model to X and y.\"\"\"\n",
    "rfe = RFE(lr, n_features_to_select=3)\n",
    "rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Result of Recursive Feature Elimination\n",
    "You can inspect the results of recursive feature elimination by looking at rfe.ranking_ and rfe.support_.\n",
    "\n",
    "rfe.ranking_ is an array that contains the rank of each feature. Here are the features from the fire dataset:\n",
    "\n",
    "['Temperature', 'RH', 'Ws', 'Rain', 'DMC', 'FWI']\n",
    "Here are the feature rankings after recursive feature elimination is done on the fire dataset.\n",
    "\n",
    "print(rfe.ranking_)\n",
    "[2 5 4 1 3 1]\n",
    "A 1 at a certain index indicates that recursive feature elimination kept the feature at the same index. In this example, the model kept the features at indices 3 and 5: Rain and FWI. The other numbers indicate at which step a feature was removed. The 5 (the highest rank in the array) at index 1 means that RH (the feature at index 1) was removed first. The 4 at index 2 means that Ws (the feature at index 2) was removed in the next step, and so on.\n",
    "\n",
    "rfe.support_ is an array with True and False values that indicate which features were chosen. Here’s an example of what this looks like, again using the fire dataset.\n",
    "\n",
    "print(rfe.support_)\n",
    "[False False False  True False  True]\n",
    "This array indicates that the features at indices 3 and 5 were chosen. The features at indices 0, 1, 2, and 4 were eliminated.\n",
    "\n",
    "If you have a list of feature names, you can use a list comprehension and rfe.support_ to get a list of chosen feature names.\n",
    "\n",
    "# features is a list of feature names\n",
    "# Get a list of features chosen by rfe\n",
    "rfe_features = [f for (f, support) in zip(features, rfe.support_) if support]\n",
    " \n",
    "print(rfe_features)\n",
    "['Rain ', 'FWI']\n",
    "You can use rfe.score(X, y) to check the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7327586206896551\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "health = pd.read_csv(\"data/dataR2.csv\")\n",
    "X = health.iloc[:,:-1]\n",
    "y = health.iloc[:,-1]\n",
    "\n",
    "\"\"\"Create a list called rfe_features that contains the names of the features chosen by recursive feature elimination.\"\"\"\n",
    "# Create a list of feature names\n",
    "feature_list = list(X.columns)\n",
    "\n",
    "# Standardize the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Logistic regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Recursive feature elimination\n",
    "rfe = RFE(estimator=lr, n_features_to_select=3)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# List of features chosen by recursive feature elimination\n",
    "rfe_features = [f for (f, support) in zip(feature_list, rfe.support_) if support]\n",
    "\n",
    "\"\"\"Print the accuracy of the model after doing recursive feature elimination. How does this compare to the accuracy of the original model?\"\"\"\n",
    "# Print the accuracy of the model with features chosen by recursive feature elimination\n",
    "print(rfe.score(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "Congratulations! You’ve learned about a few different wrapper methods. You’ve also learned how to implement them in Python and evaluate the results. Let’s recap what we covered.\n",
    "\n",
    "Wrapper methods for feature selection are algorithms that select features by evaluating the performance of a machine learning model on different subsets of features. Here are some advantages of wrapper methods.\n",
    "\n",
    "They can detect relationships between features that might be relevant to the machine learning model.\n",
    "Unlike filter methods, they are designed to choose features that are relevant to whatever machine learning model you are using.\n",
    "We covered four different greedy wrapper methods and implemented them using mlextend in Python.\n",
    "\n",
    "Sequential forward selection adds one feature at a time.\n",
    "Sequential backward selection removes one feature at a time.\n",
    "Sequential forward floating selection adds (and sometimes removes) one feature at a time.\n",
    "Sequential backward floating selection removes (and sometimes adds) one feature at a time.\n",
    "We also covered recursive feature elimination, which ranks features by importance and removes the least important feature at every step. We used the scikit-learn library to implement that algorithm and investigate the results.\n",
    "\n",
    "The forest fire data for this lesson were taken from the UCI Machine Learning Repository Faroudja ABID et al., Predicting Forest Fire in Algeria using Data Mining Techniques: Case Study of the Decision Tree Algorithms, International Conference on Advanced Intelligent Systems for Sustainable Development (AI2SD 2019) , 08 - 11 July , 2019, Marrakech, Morocco.\n",
    "\n",
    "The breast cancer data for this lesson were taken from the UCI Machine Learning Repository. [Patricio, 2018] Patrício, M., Pereira, J., Crisóstomo, J., Matafome, P., Gomes, M., Seiça, R., & Caramelo, F. (2018). Using Resistin, glucose, age and BMI to predict the presence of breast cancer. BMC Cancer, 18(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8810d01d3690a4af1432156f3833ae05339dfbc619bed7f8ee09aae71d8076a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
